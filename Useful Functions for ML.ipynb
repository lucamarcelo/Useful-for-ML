{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greek-commander",
   "metadata": {},
   "source": [
    "# Useful functions for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-carroll",
   "metadata": {},
   "source": [
    "## 1 EDA \n",
    "### 1.1 Visualization\n",
    "#### Printing histograms/barplots for columns in df\n",
    "This isn't really a function, but is very useful to print numerical or categorical variables with a histogram or barplot respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-telling",
   "metadata": {},
   "source": [
    "#### Target variables (y)\n",
    "In this case we have two target variables: \n",
    "- `h1n1_vaccine` - Whether respondent received H1N1 flu vaccine.\n",
    "- `seasonal_vaccine` - Whether respondent received seasonal flu vaccine.\n",
    "\n",
    "#### Features (X)\n",
    "For all binary variables: 0 = No; 1 = Yes.\n",
    "\n",
    "- `h1n1_concern` - Level of concern about the H1N1 flu.<br> 0 = Not at all concerned; 1 = Not very concerned; 2 = Somewhat concerned; 3 = Very concerned.\n",
    "- `h1n1_knowledge` - Level of knowledge about H1N1 flu.<br> 0 = No knowledge; 1 = A little knowledge; 2 = A lot of knowledge.\n",
    "- `behavioral_antiviral_meds` - Has taken antiviral medications. (binary)\n",
    "- `behavioral_avoidance` - Has avoided close contact with others with flu-like symptoms. (binary)\n",
    "- `behavioral_face_mask` - Has bought a face mask. (binary)\n",
    "- `behavioral_wash_hands` - Has frequently washed hands or used hand sanitizer. (binary)\n",
    "- `behavioral_large_gatherings` - Has reduced time at large gatherings. (binary)\n",
    "- `behavioral_outside_home` - Has reduced contact with people outside of own household. (binary)\n",
    "- `behavioral_touch_face` - Has avoided touching eyes, nose, or mouth. (binary)\n",
    "- `doctor_recc_h1n1` - H1N1 flu vaccine was recommended by doctor. (binary)\n",
    "- `doctor_recc_seasonal` - Seasonal flu vaccine was recommended by doctor. (binary)\n",
    "- `chronic_med_condition` - Has any of the following chronic medical conditions: asthma or an other lung condition, diabetes, a heart condition, a kidney condition, sickle cell anemia or other anemia, a neurological or neuromuscular condition, a liver condition, or a weakened immune system caused by a chronic illness or by medicines taken for a chronic illness. (binary)\n",
    "- `child_under_6_months` - Has regular close contact with a child under the age of six months. (binary)\n",
    "- `health_worker` - Is a healthcare worker. (binary)\n",
    "- `health_insurance` - Has health insurance. (binary)\n",
    "- `opinion_h1n1_vacc_effective` - Respondent's opinion about H1N1 vaccine effectiveness.<br> 1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective.\n",
    "- `opinion_h1n1_risk` - Respondent's opinion about risk of getting sick with H1N1 flu without vaccine. <br> 1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high.\n",
    "- `opinion_h1n1_sick_from_vacc` - Respondent's worry of getting sick from taking H1N1 vaccine.<br> 1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried.\n",
    "- `opinion_seas_vacc_effective` - Respondent's opinion about seasonal flu vaccine effectiveness.<br> 1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective.\n",
    "- `opinion_seas_risk` - Respondent's opinion about risk of getting sick with seasonal flu without vaccine.<br> 1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high.\n",
    "- `opinion_seas_sick_from_vacc` - Respondent's worry of getting sick from taking seasonal flu vaccine.<br> 1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried.\n",
    "- `age_group` - Age group of respondent.\n",
    "- `education` - Self-reported education level.\n",
    "- `race` - Race of respondent.\n",
    "- `sex` - Sex of respondent.\n",
    "- `income_poverty` - Household annual income of respondent with respect to 2008 Census poverty thresholds.\n",
    "- `marital_status` - Marital status of respondent.\n",
    "- `rent_or_own` - Housing situation of respondent.\n",
    "- `employment_status` - Employment status of respondent.\n",
    "- `hhs_geo_region` - Respondent's residence using a 10-region geographic classification defined by the U.S. Dept. of Health and Human Services. Values are represented as short random character strings.\n",
    "- `census_msa` - Respondent's residence within metropolitan statistical areas (MSA) as defined by the U.S. Census.\n",
    "- `household_adults` - Number of other adults in household, top-coded to 3.\n",
    "- `household_children` - Number of children in household, top-coded to 3.\n",
    "- `employment_industry` - Type of industry respondent is employed in. Values are represented as short random character strings.\n",
    "- `employment_occupation` - Type of occupation of respondent. Values are represented as short random character strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spiritual-hospital",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f06687e1e49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#For numerical variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## Version1 - creates various scrollable plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_num' is not defined"
     ]
    }
   ],
   "source": [
    "#For numerical variables\n",
    "## Version1 - creates various scrollable plots \n",
    "for col in x_num.columns: \n",
    "    plt.hist(x_num[col])\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "    \n",
    "## Version2 - creates a canvas with all plots\n",
    "x_num.hist(figsize = (len(x_num), len(x_num)))\n",
    "\n",
    "## Version 3 \n",
    "df_numerical = df.select_dtypes(include=np.number)\n",
    "df[df_numerical.columns].hist(bins=15, figsize=(15, 6), layout=(2, 4)) \n",
    "    \n",
    "#For categorical variables\n",
    "## Version 1 - creates various plots you can scroll through \n",
    "for col in x_cat.columns:\n",
    "    sns.barplot(x_cat[col].value_counts().index, x_cat[col].value_counts()).set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "## Version 2 - creates a grid of the different plots (preferrable, since it's more overviewable)\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for variable, subplot in zip(categorical, ax.flatten()):\n",
    "    sns.countplot(housing[variable], ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-receptor",
   "metadata": {},
   "source": [
    "#### Unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "def unique_per_col(df): \n",
    "    #Selecting only categorical cols\n",
    "    cols = (df.dtypes == 'object')\n",
    "    object_cols = list(cols[cols].index)\n",
    "    #Count unique values\n",
    "    object_nunique = list(map(lambda col: df[col].nunique(), object_cols))\n",
    "    d = dict(zip(object_cols, object_nunique))\n",
    "    #Print number of unique entries by column, in ascending order\n",
    "    print('Unique values per category:')\n",
    "    print('='* len(cols))\n",
    "    for row in sorted(d.items(), key=lambda x: x[1]):\n",
    "        print(row)\n",
    "        \n",
    "# ########################################################################## \n",
    "\n",
    "# Alternatively: output won't be sorted\n",
    "def unique_per_col2(df): \n",
    "    # Selecting only categorical cols\n",
    "    cat_cols = (df.dtypes == 'object')\n",
    "    print('Unique values per category:'); print('='*len('Unique values per category:'))\n",
    "    for var in cat_cols.columns: \n",
    "        print(var, len(cat_cols[var].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-georgia",
   "metadata": {},
   "source": [
    "#### Create dataframes for categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concrete-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical and numerical data of a df and creating two new dfs for both datatypes\n",
    "df_num =  df.select_dtypes(include=np.number)\n",
    "df_cat = df.select_dtypes(exclude=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-intake",
   "metadata": {},
   "source": [
    "## 2 Pre-processing \n",
    "### 2.1 Missing Values\n",
    "#### Calculate missing values by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per variable: Dtypes, unique values, nan%, zeros%, first 5, last 5\n",
    "def explore(data):\n",
    "    df = pd.DataFrame()\n",
    "    for col in list(data):\n",
    "        unique_values = data[col].unique()\n",
    "        try:\n",
    "            unique_values = np.sort(unique_values)\n",
    "        except:\n",
    "            pass\n",
    "        nans = round(pd.isnull(data[col]).sum()/data.shape[0]*100, 1)\n",
    "        zeros = round((data[col] == 0).sum()/data.shape[0]*100, 1)\n",
    "        dtypes = \", \".join(data[col].map(lambda val: val.__class__.__name__).unique())\n",
    "        df = df.append(pd.DataFrame([col,\n",
    "                                     dtypes,\n",
    "                                     len(unique_values),\n",
    "                                     nans,\n",
    "                                     zeros,\n",
    "                                     unique_values[:5],\n",
    "                                     unique_values[-5:]]).T,\n",
    "                       ignore_index = True)\n",
    "    return df.rename(columns = {0: \"variable\",\n",
    "                                1: \"dtypes\",\n",
    "                                2: \"unique_values\",\n",
    "                                3: \"Nan %\",\n",
    "                                4: \"zeros %\",\n",
    "                                5: \"first_5\",\n",
    "                                6: \"last_5\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values per column\n",
    "missingno.matrix(df, figsize = (30,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-current",
   "metadata": {},
   "source": [
    "#### Dropping columns with NAs > treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-complexity",
   "metadata": {},
   "source": [
    "## 3 Feature Selection / Feature Engineering\n",
    "### 3.1 Correlations of features\n",
    "####  3.1.1 Correlation between Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-annex",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40482bfa687a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "    \n",
    "# Visualize\n",
    "pyplot.scatter(x, y)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-lesson",
   "metadata": {},
   "source": [
    "#### Detecting Multicollinearity using pairplots\n",
    "A pairplot or scatter matrix plots all numerical variables against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(traindf[features], figsize = (12, 12), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-wilderness",
   "metadata": {},
   "source": [
    "#### Detecting Multicollinearity using VIF \n",
    "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "retired-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-blackberry",
   "metadata": {},
   "source": [
    "#### Remove collinear variables according to a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between Energy Star Score\n",
    "    y = x['score']\n",
    "    x = x.drop(columns = ['score'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu/ft²)', \n",
    "                          'Water Use (All Water Sources) (kgal)',\n",
    "                          'log_Water Use (All Water Sources) (kgal)',\n",
    "                          'Largest Property Use Type - Gross Floor Area (ft²)'])\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['score'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-edinburgh",
   "metadata": {},
   "source": [
    "- VIF starts at 1 and has no upper limit\n",
    "- VIF = 1, no correlation between the independent variable and the other variables\n",
    "- VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-dream",
   "metadata": {},
   "source": [
    "#### 3.1.2 Correlation between Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cramer's V - Symmetrical\n",
    "## - 0 = no association, 1 = full association, no negative values\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theil's U - Asymmetrical (U(x,y)≠U(y,x))\n",
    "## - 0 = no association, 1 = full association, no negative values\n",
    "##########################\n",
    "# Preferable to use, since it shows the correlations in a clearer way and doesn't show \n",
    "# two way correlations\n",
    "##########################\n",
    "\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-telephone",
   "metadata": {},
   "source": [
    "#### 3.1.3 Correlation between Categorical AND Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = np.sqrt(numerator/denominator)\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-metallic",
   "metadata": {},
   "source": [
    "### 3.2 Correlation with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates correlation with target of a whole df \n",
    "df.corrwith(df[\"Target\"])\n",
    "\n",
    "### EXAMPLE: ### with a subset of features rather than the whole dataframe\n",
    "# df[['Income', 'Education', 'LoanAmount']].corr()['LoanAmount'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-increase",
   "metadata": {},
   "source": [
    "#### Variance for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proporciones_final (var,target,df):\n",
    "    proporcion = pd.DataFrame()\n",
    "    \n",
    "    proporcion['%depositos'] = df[target].groupby(df[var]).mean()*100\n",
    "    proporcion['Conteo'] = df[target].groupby(df[var]).count()\n",
    "    proporcion= proporcion.round(3)   \n",
    "    proporcion_filtered = proporcion[(proporcion['%depositos']>0) & (proporcion['Conteo']>10)]\n",
    "        \n",
    "    if len(proporcion_filtered)<100 and len(proporcion_filtered)>1:\n",
    "        fig = plt.figure()\n",
    "        ax = proporcion_filtered['Conteo'].plot(kind='bar',grid=True)\n",
    "                \n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(proporcion_filtered['%depositos'].values, linestyle='-', linewidth=2.0,color='g')\n",
    "        plt.tight_layout()        \n",
    "    else:        \n",
    "        proporcion_filtered.reset_index(inplace=True)\n",
    "        sns.lmplot(x = var,y ='%depositos',data=proporcion_filtered, fit_reg=True,ci=None)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-twelve",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, data):\n",
    "    \"\"\"\n",
    "    Function to show which features are most important in the model.\n",
    "    ::param_model:: Which model to use?\n",
    "    ::param_data:: What data to use?\n",
    "    \"\"\"\n",
    "    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n",
    "    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n",
    "    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n",
    "    return fea_imp\n",
    "    #plt.savefig('catboost_feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-cheat",
   "metadata": {},
   "source": [
    "#### Mutual information\n",
    "Here are some things to remember when applying mutual information:\n",
    "- MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\n",
    "- It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\n",
    "- The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.\n",
    "\n",
    "––> **Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects -- domain knowledge can offer a lot of guidance here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regressio\n",
    "# for classification: from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores\n",
    "\n",
    "# ##################\n",
    "# Visualize the data\n",
    "# ##################\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-necklace",
   "metadata": {},
   "source": [
    "## 4 Evaluation\n",
    "#### Evaluation  Metrics for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "involved-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to add average=None for multiclass classification\n",
    "def classifier_metrics (y1,   #y_test\n",
    "                        y2):  #y_pred\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y1, y2))\n",
    "    print('Accuracy')\n",
    "    print(accuracy_score(y1, y2))\n",
    "    print('Precision')\n",
    "    print(precision_score(y1, y2))\n",
    "    print('Recall')\n",
    "    print(recall_score(y1, y2))\n",
    "    print('f1')\n",
    "    print(f1_score(y1, y2))\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y1, y2)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    print('AUC')\n",
    "    print(roc_auc)\n",
    "    plt.plot(false_positive_rate, recall, 'b')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.title('AUC = %0.2f' % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-medline",
   "metadata": {},
   "source": [
    "#### Evaluation Metric for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-classic",
   "metadata": {},
   "source": [
    "## 5 Others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show progress of calculation\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "mb = master_bar(range(10))\n",
    "for i in mb:\n",
    "    for j in progress_bar(range(100), parent=mb):\n",
    "        sleep(0.01)\n",
    "        mb.child.comment = f'second bar stat'\n",
    "    mb.first_bar.comment = f'first bar stat'\n",
    "    mb.write(f'Finished loop {i}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pandas quicker by importing modin.pandas \n",
    "import modin.pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
