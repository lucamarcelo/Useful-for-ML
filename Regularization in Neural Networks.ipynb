{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wireless-deadline",
   "metadata": {},
   "source": [
    "## Regularization techniques in Keras - a basic overview \n",
    "Overfitting is an big problem when training a model and it occurs when the model achieves a very high accuracy on the training data, i.e. has learned the training data's features very well (and sometimes even remembers every single datapoint), but fails to achieve a similar accuracy on new (test) data. \n",
    "\n",
    "This is a problem, since what we actually care about is how well the model predicts *new* data, for which we don't have an answer in regards to which class it belongs to, for example in the case of a classification problem. \n",
    "For the training data we already *have* the answer, therefore it's accuracy isn't as important. \n",
    "\n",
    "There's a lot of discussion about whether over- or underfitting is worse (make sure to check out the [question](https://stats.stackexchange.com/questions/521835/is-overfitting-better-than-underfitting) I asked out on Cross-Validated), the key is to find the sweetspot between both. \n",
    "\n",
    "![Graph](/Users/luca/Desktop/Screenshot.png)\n",
    "\n",
    "In the following we're going to focus on some regularization techniques to prevent overfitting in neural networks.\n",
    "**Note**: Here I will mainly focus on the code and how to implement it using Keras, and not as much on the theoretical background. \n",
    "\n",
    "### Overview of regularization\n",
    "#### 1) Create a simpler model\n",
    "Often times a model that is too complex (in this case we will refer to complexity as a high number of parameters), for example to deep or too wide, will overfit. On the flipside, a model that isn't complex *enough* won't learn form the data and underfit. So the first thing we should ask ourselves is of we can reduce model complexity by reducing the number of parameters. \n",
    "\n",
    "#### 2) Reducing batch_size\n",
    "Lowering the batchsize when we fit the training data to the model, i.e. train the model, creates more uncertainty and can help to reduce overfitting. For example a model with batch_size = 500 will be more likely to overfit than a mode with batch_size = 100. \n",
    "\n",
    "#### 3) Dropout\n",
    "This is likely to be the most effective way of reducing overfitting. \n",
    "Dropout means randomly ignoring or “dropping out” some number of layer outputs.\n",
    "Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n",
    "\n",
    "#### 4) Early Stopping\n",
    "Early stopping means exactly what it sounds like: We stop the model before it overfits. \n",
    "Usually with increasing training time the model will aproximate 100% training accuracy while test test accuracy drops. Therefore sometimes by stopping the model earlier than the actual epochs, we can prevent overfitting. \n",
    "\n",
    "#### 4) Batch Normalization\n",
    "Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. Batch normalization is mostly effective for very deep neural networks. \n",
    "\n",
    "#### 5) Add penalty to cost function - L1, L2 and elastic net regularizer\n",
    "Neural networks learn a set of weights that best map inputs to outputs.\n",
    "\n",
    "A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data.\n",
    "\n",
    "A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model.\n",
    "\n",
    "#### 6) Get more training data\n",
    "This can be tricky at times, because getting data may be expensive, unaccessible or simply not available. \n",
    "But generally, the more data we have, the better the model can estimate the \"real\" value of the parameters. \n",
    "\n",
    "\n",
    "\n",
    "*For a simple example:* <br>\n",
    "Imagine you wanted to estimate the population mean of the height of students at a university.\n",
    "If you would pick a sample of 5 students your sample mean will probably vary greatly from the \"real\" (the population) mean. Therefore estimating the population mean from only a sample of 5 students results in high variance. <br>\n",
    "On the otherhand if you had a sample of 500 students and wanted to estimate the population mean from this sample mean, you could be more much confident about not having as much variance. \n",
    "\n",
    "***What are alternative ways to get more data and make the model more robust?***\n",
    "\n",
    "#### 7) Data Augmentation \n",
    "This means creating a duplicate with small modifications for each datapoint in the set. \n",
    "For example, for a training set with dog pictures, we augment the data by cropping, brightening, zooming in, darkening or mirroring the picture. \n",
    "\n",
    "#### 8) Noise injection\n",
    "*Why are small datasets a problem?*\n",
    "- The first problem is that the model might memorize the training set entirely. This means it will learn each speficic input and its associated output, when it should learn a general mapping of how inputs and outputs. \n",
    "- The second problem is that a small dataset provides less opportunity to describe the structure of the input space and its relationship to the output. More training data provides a richer description of the problem from which the model may learn. Fewer data points means that rather than a smooth input space, the points may represent a jarring and disjointed structure that may result in a difficult, if not unlearnable, mapping function.\n",
    "\n",
    "Adding random noise then will help to smoothen out those datapoints on create a larger input space.\n",
    "\n",
    "\n",
    "### Implementation in Keras\n",
    "We will work with the mnist dataset, which is a well-known dataset consisting of a collection of 28x28 pixel images corresponding in digits from 0 to 9 manuscripts. The purpose of this data set is to train models that recognize handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# As we've said, the images consist of a 28x28 matrix (that is the pixels).\n",
    "# Each pixel is encoded to its corresponding color channel (in this case only gray), \n",
    "# with values between 0 (black) and 255 (white). \n",
    "\n",
    "# It is usual to normalize the values to work with a range between 0.0 and 1.0.\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "# split into train test sets\n",
    "# reduce the dataset to induce more overfitting\n",
    "_, x, _, y = train_test_split(\n",
    "    x_train, y_train, test_size=0.02, random_state=1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-finding",
   "metadata": {},
   "source": [
    "#### Baseline model (without regularization) \n",
    "We'll start out building our baseline model without any regularization consisting of our inputs flattened to a vector (since we cannot feed a matrix into a [feed-forward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network)).\n",
    "\n",
    "To build the model we will use the functional api, as opposed to the [sequential model](https://hanifi.medium.com/sequential-api-vs-functional-api-model-in-keras-266823d7cd5e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "base_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "base_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = base_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = base_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-subscription",
   "metadata": {},
   "source": [
    "### Regularize Baseline Model\n",
    "#### 1) Simpler Model\n",
    "First and easiest thing to do is always to ask yourself if we can create a simpler model without compromising its ability to learn. Practically this means **reducing the number of parameters** by either removing neurons or layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler model -- remove third layer and reduce number of neurons in the remaining\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(80, activation='relu')(flat)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(60, activation='relu')(l_1)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_2)\n",
    "\n",
    "## Model definition\n",
    "simple_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "simple_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_2 = simple_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results_2 = simple_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results_2[0]))\n",
    "print('Test Accuracy: {}'.format(results_2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-portland",
   "metadata": {},
   "source": [
    "#### 2) Reduce `batch_size`\n",
    "`batch_size` is a parameter when we fit the model and is the number of training samples that goes through one forward (or backward) pass. After each pass the internal model parameters (the weights and biases) are updated. \n",
    "Larger `batch_sizes` require more memory, while smaller ones fluctuate more (since the parameters get updated more frequently), which is meant by they introduce \"uncertainty\".\n",
    "\n",
    "*The important thing is*: It can help to prevent overfitting, but keep in mind that it will also result in longer training times. <br>\n",
    "In practice it's literally as easy as reducing the number in the `batch_size` parameter when fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced batch_size model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "redbatch_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "redbatch_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = redbatch_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=40, # In practice it's as simple as reducing the batch size from let's say 80 to a lower number, e.g. 40\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = redbatch_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-performer",
   "metadata": {},
   "source": [
    "#### 3) Dropout\n",
    "This is a technique that's exceptionally effective.<br>\n",
    "In practice, we \"switch off\" some neurons in the model with a probability of p.\n",
    "This is done by adding *dropout layers* to the model.<br>\n",
    "In this case we'll go with a probability of p=0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "#Adding dropout layer\n",
    "flat = layers.Dropout(0.5, name='dropout_flat')(flat)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "l_1 = layers.Dropout(0.5, name='dropout_l1')(l_1)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "l_2 = layers.Dropout(0.5, name='dropout_l2')(l_2)\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "l_3 = layers.Dropout(0.5, name='dropout_l3')(l_3)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "dropout_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "dropout_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = dropout_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = dropout_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-interest",
   "metadata": {},
   "source": [
    "#### 4) Early Stopping\n",
    "Oftentimes we see that with increasing training time the model will start to approximate 1 on the training data and get lower accuracy on the test data. <br>\n",
    "We can avoid this by stopping the training at the point where the difference between train and test data is the lowest (the \"sweetspot\" mentioned before). Early stopping also results in shorter training times. \n",
    "\n",
    "To perform early stopping we will use the Keras Callbacks :\n",
    "\n",
    "`tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    ")`\n",
    "\n",
    "The model architecture is basically the same, we just have to specify the callback and pass it as an argument when we fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "es_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying callback\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "es_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = es_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    callback = [es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = es_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-stylus",
   "metadata": {},
   "source": [
    "#### 5) Batch Normalization \n",
    "Batch normalization applies a transformation that keeps the mean close to 0 and the standard deviation close to 1. \n",
    "In Keras we simply add an extra layer for BatchNorm to each existing layer, similar to what we do with the Dropout layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "l_1 = layers.BatchNormalization()(l_1) # adding the BatchNorm layer \n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "l_2 = layers.BatchNormalization()(l_2) # adding the BatchNorm layer\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "l_3 = layers.BatchNormalization()(l_3) # adding the BatchNorm layer\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "batchnorm_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "batchnorm_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit = batchnorm_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = batchnorm_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-facility",
   "metadata": {},
   "source": [
    "#### 5) L1, L2 and Elastic Net\n",
    "In Keras, we just **create the kernels and pass them as an argument in the layer**.\n",
    "\n",
    "Furthermore, it is possible to choose whether to include the penalty in the cost function on the weights, the biases or on the activation, with the following arguments:\n",
    "\n",
    "`kernel_regularizer: only on weights.\n",
    "bias_regularizer: only on biases.\n",
    "activity_regularizer: on full output.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating kernels\n",
    "L1_regularizer = regularizers.l1(1e-5)\n",
    "L2_regularizer = regularizers.l2(5e-4)\n",
    "elastic_net = regularizers.l1_l2(l1=1e-5, l2=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizer model - here we will just only use the L2 regularizer which is usually the one working best\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu', kernel_regularizer=L2_regularizer)(flat)\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu', kernel_regularizer=L2_regularizer)(l_1)\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu', kernel_regularizer=L2_regularizer)(l_2)\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "regularizers_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "regularizers_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = regularizers_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = regularizers_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-strap",
   "metadata": {},
   "source": [
    "#### 6) Noise Injection\n",
    "Adding random noise then will help to smoothen out those datapoints on create a larger input space.\n",
    "\n",
    "In Keras we **just add a noise layer**, similar to what we did with the Dropout layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizer model - here we will just only use the L2 regularizer which is usually the one working best\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "stddev = 2 # specific SD for noise\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat)\n",
    "l_1 = layers.GaussianNoise(stddev, name='noise_l1')(l_1) # add noise layer\n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu', kernel_regularizer=L2_regularizer)(l_1)\n",
    "l_2 = layers.GaussianNoise(stddev, name='noise_l2')(l_2) # add noise layer\n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu', kernel_regularizer=L2_regularizer)(l_2)\n",
    "l_3 = layers.GaussianNoise(stddev, name='noise_l3')(l_3) # add noise layer\n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "noise_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "noise_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = noise_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = noise_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-parameter",
   "metadata": {},
   "source": [
    "#### Combining different methods\n",
    "Of course we can also combine different regularizers in one and the same model. \n",
    "<br>\n",
    "For example combining **dropout with batchnorm** like in the following: <br><br>\n",
    "NOTE: There's some discussion about what the order of the layers should be, i.e. of BatchNorm should be applied before or after dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout model\n",
    "\n",
    "## Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "## Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "#Adding dropout layer\n",
    "flat = layers.Dropout(0.5, name='dropout_flat')(flat)\n",
    "\n",
    "## hidden layer 1\n",
    "l_1 = layers.Dense(128, activation='relu')(flat) # dense layer \n",
    "l_1 = layers.BatchNormalization()(l_1)           # BatchNorm layer \n",
    "l_1 = layers.Dropout(0.5, name='dropout_l1')(l_1)# Dropout layer \n",
    "## hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "l_2 = layers.BatchNormalization()(l_2)           # BatchNorm layer \n",
    "l_2 = layers.Dropout(0.5, name='dropout_l2')(l_2)# Dropout layer \n",
    "## hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "l_3 = layers.BatchNormalization()(l_3)           # BatchNorm layer \n",
    "l_3 = layers.Dropout(0.5, name='dropout_l3')(l_3)# Dropout layer \n",
    "\n",
    "## Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "## Model definition\n",
    "combined_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "combined_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training\n",
    "fit_1 = combined_model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=80,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = combined_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
